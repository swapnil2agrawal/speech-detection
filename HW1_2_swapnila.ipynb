{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print (cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class WSJ():\n",
    "    \"\"\" Load the WSJ speech dataset\n",
    "        \n",
    "        Ensure WSJ_PATH is path to directory containing \n",
    "        all data files (.npy) provided on Kaggle.\n",
    "        \n",
    "        Example usage:\n",
    "            loader = WSJ()\n",
    "            trainX, trainY = loader.train\n",
    "            assert(trainX.shape[0] == 24590)\n",
    "            \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.dev_set = None\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "  \n",
    "    @property\n",
    "    def dev(self):\n",
    "        if self.dev_set is None:\n",
    "            self.dev_set = load_raw(os.environ['./11-785hw1p2-f19/'], 'dev')\n",
    "        return self.dev_set\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.train_set is None:\n",
    "            self.train_set = load_raw(os.environ['./11-785hw1p2-f19/'], 'train')\n",
    "        return self.train_set\n",
    "  \n",
    "    @property\n",
    "    def test(self):\n",
    "        if self.test_set is None:\n",
    "            self.test_set = (np.load(os.path.join(os.environ['./11-785hw1p2-f19/'], 'test.npy'), encoding='bytes'), None)\n",
    "        return self.test_set\n",
    "    \n",
    "def load_raw(path, name):\n",
    "    return (\n",
    "        np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes', allow_pickle=True), \n",
    "        np.load(os.path.join(path, '{}_labels.npy'.format(name)), encoding='bytes', allow_pickle=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 24500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = load_raw('./11-785hw1p2-f19/', 'train')\n",
    "np.shape(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = load_raw('./11-785hw1p2-f19/', 'dev')\n",
    "np.shape(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.load('./11-785hw1p2-f19/test.npy', encoding='bytes', allow_pickle=True)\n",
    "np.shape(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = train_data\n",
    "val, val_labels = valid_data\n",
    "test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the whole dataset\n",
    "\n",
    "concat_train = np.concatenate(train) #[i] for i in range(len(train))  \n",
    "concat_train_labels = np.concatenate(train_labels) #[i] for i in range(len(trainlabels))\n",
    "concat_val = np.concatenate(val) #[i] for i in range(len(trainlabels)\n",
    "concat_valid_labels = np.concatenate(val_labels) #[i] for i in range(len(trainlabels)\n",
    "concat_test = np.concatenate(test) #[i] for i in range(len(test))                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15388713, 40)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(concat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapframe for train data\n",
    "mapframe = {}\n",
    "mapframe[-1] = 0\n",
    "for i in range(len(train)):\n",
    "    mapframe[i] = len(train[i]) + mapframe[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapframe_valid = {}\n",
    "mapframe_valid[-1] = 0\n",
    "for i in range(len(val)):\n",
    "    mapframe_valid[i] = len(val[i]) + mapframe_valid[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for train padding\n",
    "\n",
    "dict = {}\n",
    "len_train = len(train)\n",
    "k = 12\n",
    "\n",
    "for i in range(len_train):\n",
    "#     print(i)\n",
    "    #corresponding to every index, store how many elements to pad and before(0) or after(1)\n",
    "    for j in range(mapframe[i-1], mapframe[i-1]+ k):\n",
    "        dict[j] = [k - (j - mapframe[i-1]), 0]  \n",
    "        \n",
    "    for j in range(mapframe[i]-k, mapframe[i]):\n",
    "        dict[j] = [1 + k- (mapframe[i] - j), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_val = {}\n",
    "len_val = len(val)\n",
    "k = 12\n",
    "\n",
    "for i in range(len_val):\n",
    "#     print(i)\n",
    "    #corresponding to every index, store how many elements to pad and before(0) or after(1)\n",
    "    for j in range(mapframe_valid[i-1], mapframe_valid[i-1]+ k):\n",
    "        dict_val[j] = [k - (j - mapframe_valid[i-1]), 0]  \n",
    "        \n",
    "    for j in range(mapframe_valid[i]-k, mapframe_valid[i]):\n",
    "        dict_val[j] = [1 + k- (mapframe_valid[i] - j), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class joinframe(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y, k):\n",
    "        super().__init__()\n",
    "        assert len(x) == len(y)\n",
    "        self._x = x\n",
    "        self._y = y     \n",
    "        self.k = k\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._x)\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "#         print(\"index: \", index)\n",
    "        y_item = self._y[index]\n",
    "        \n",
    "        if index in dict :\n",
    "            len_pad = dict[index][0]\n",
    "            \n",
    "            if dict[index][1] == 0:\n",
    "                # before padding\n",
    "#                 print(\"len_pad: \", dict[index][0])\n",
    "                \n",
    "                x_item = self._x[(index - (self.k - len_pad)) : (index + self.k+1)]\n",
    "#                 print(\"data:\", x_item.shape)\n",
    "                padding = np.zeros((dict[index][0], 40))\n",
    "#                 print(\"padding:\", padding.shape)\n",
    "                x_item = np.vstack((padding, x_item))\n",
    "                x_item = x_item.flatten()             \n",
    "          \n",
    "            elif dict[index][1] == 1:\n",
    "                # after padding\n",
    "                x_item = self._x[(index - self.k) : (index + 1 + (self.k-len_pad))]\n",
    "                padding = np.zeros((dict[index][0], 40))\n",
    "                x_item = np.vstack((x_item, padding))\n",
    "                x_item = x_item.flatten()\n",
    "                \n",
    "        else:\n",
    "            x_item = self._x[index - self.k : index + self.k + 1]\n",
    "            x_item = x_item.flatten()\n",
    "\n",
    "        return x_item, y_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameobj = joinframe(concat_train, concat_train_labels, k = 12) \n",
    "framedata = DataLoader(frameobj, batch_size=1024, shuffle=True, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANN(\n",
       "  (linear1): Linear(in_features=1000, out_features=2048, bias=True)\n",
       "  (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (linear3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (linear4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear5): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (linear6): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (linear7): Linear(in_features=256, out_features=138, bias=True)\n",
       "  (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (do): Dropout(p=0.2)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, k = 12, output_size = 138):\n",
    "        super(ANN, self).__init__()\n",
    "        input_size = 40*(2*k+1)\n",
    "        self.hiddens = [2048, 1024, 1024, 512, 512, 256]\n",
    "#         self.hiddens = [1024, 1024, 512, 512, 256]\n",
    "        #all linear layer\n",
    "        self.linear1 = nn.Linear(input_size, self.hiddens[0])\n",
    "        self.linear2 = nn.Linear(self.hiddens[0], self.hiddens[1])\n",
    "        self.linear3 = nn.Linear(self.hiddens[1], self.hiddens[2])\n",
    "        self.linear4 = nn.Linear(self.hiddens[2], self.hiddens[3])\n",
    "        self.linear5 = nn.Linear(self.hiddens[3], self.hiddens[4])\n",
    "        self.linear6 = nn.Linear(self.hiddens[4], self.hiddens[5])\n",
    "        self.linear7 = nn.Linear(self.hiddens[5], output_size)\n",
    "#         self.linear5 = nn.Linear(self.hiddens[3], output_size)\n",
    "    \n",
    "        #all batch_norm layer\n",
    "        self.bn1 = nn.BatchNorm1d(self.hiddens[0])\n",
    "        self.bn2 = nn.BatchNorm1d(self.hiddens[1])\n",
    "        self.bn3 = nn.BatchNorm1d(self.hiddens[2])\n",
    "        self.bn4 = nn.BatchNorm1d(self.hiddens[3])\n",
    "        self.bn5 = nn.BatchNorm1d(self.hiddens[4])\n",
    "        self.bn6 = nn.BatchNorm1d(self.hiddens[5]) \n",
    "        \n",
    "        self.do = nn.Dropout(p=0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, example):\n",
    "        \n",
    "        x = example\n",
    "        x = self.relu(self.bn1(self.linear1(x)))\n",
    "        x = self.relu(self.bn2(self.linear2(x)))\n",
    "        x = self.relu(self.bn3(self.linear3(x)))\n",
    "        x = self.relu(self.bn4(self.linear4(x)))\n",
    "        x = self.relu(self.bn5(self.linear5(x)))\n",
    "        x = self.relu(self.bn6(self.linear6(x)))\n",
    "        out = self.linear7(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def init_randn(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "    \n",
    "model = ANN()\n",
    "model.double()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), 0.001, momentum = 0.1, weight_decay = 1e-8)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-bf17509b5eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mycap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 20\n",
    "train_accuracy=[]\n",
    "valid_accuracy=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    print('Epoch: ',epoch)\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    runtime = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    frameobj = joinframe(concat_train, concat_train_labels, k = 12) \n",
    "    framedata = DataLoader(frameobj, batch_size= 512, shuffle=True, pin_memory=True, num_workers=0)\n",
    "\n",
    "    for xbatch, ybatch in framedata:\n",
    "        runtime+=1\n",
    "        if runtime%1000==0:\n",
    "            print('---------------------------',runtime)\n",
    "\n",
    "        xbatch = xbatch.to(device)\n",
    "        ybatch = ybatch.to(device)\n",
    "        if len(xbatch) < 2:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ycap = model(xbatch)\n",
    "        loss = loss_function(ycap, ybatch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        train_predict = torch.argmax(ycap.data, 1)\n",
    "        correct+=(train_predict==ybatch).sum().item()\n",
    "\n",
    "        total+=ybatch.size(0)\n",
    "    \n",
    "    print(time.time() - start)\n",
    "    accuracy = 100*correct/total\n",
    "    train_accuracy.append(accuracy)\n",
    "    print('train_accuracy = ', accuracy)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    frameobj_val = joinframe(concat_valid, concat_valid_labels, k = 12) \n",
    "    framedata_val = DataLoader(frameobj_val, batch_size= 512, shuffle=True, pin_memory=True, num_workers=0)\n",
    "    \n",
    "    for xbatch, ybatch in framedata_val:\n",
    "        \n",
    "        xbatch = xbatch.to(device)\n",
    "        ybatch = ybatch.to(device)\n",
    "        if len(xbatch) < 2:\n",
    "            continue\n",
    "\n",
    "        ycap = model(xbatch)\n",
    "        val_predict = torch.argmax(ycap.data, 1)\n",
    "        correct+=(train_predict==ybatch).sum().item()\n",
    "\n",
    "        total+=ybatch.size(0)\n",
    "    \n",
    "    print(time.time() - start)\n",
    "    accuracy = 100*correct/total\n",
    "    valid_accuracy.append(accuracy)\n",
    "    print('valid_accuracy = ', accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56.29520805281118,\n",
       " 58.547644627591666,\n",
       " 59.86857380470998,\n",
       " 60.815690045034955,\n",
       " 61.552619767488025,\n",
       " 62.13710009407544,\n",
       " 62.624229849500736,\n",
       " 63.04267289928664,\n",
       " 63.41198253551158,\n",
       " 63.72363952723012,\n",
       " 64.0239895305085,\n",
       " 64.27421188503548,\n",
       " 64.50647302344257,\n",
       " 64.73094923532592,\n",
       " 64.92802224591492,\n",
       " 65.12117680016516,\n",
       " 65.28875416677145,\n",
       " 65.45496039857264,\n",
       " 65.61492179365487,\n",
       " 65.75520642954352,\n",
       " 65.86845176721405,\n",
       " 66.01080935098341,\n",
       " 66.13293782267561,\n",
       " 66.25350021148617,\n",
       " 66.34952513572773]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 0, 0: 490, 1: 909, 2: 1441, 3: 1943, 4: 2423, 5: 2895, 6: 3387, 7: 3857, 8: 4327, 9: 4814, 10: 5259, 11: 5729, 12: 6202, 13: 6699, 14: 7123, 15: 7560, 16: 8028, 17: 8458, 18: 9003, 19: 9671, 20: 10304, 21: 10690, 22: 11017, 23: 11794, 24: 12848, 25: 13517, 26: 13839, 27: 14079, 28: 14693, 29: 14988, 30: 16069, 31: 16760, 32: 17137, 33: 17300, 34: 17429, 35: 17975, 36: 18772, 37: 19471, 38: 20178, 39: 20777, 40: 21779, 41: 22216, 42: 22988, 43: 23303, 44: 23732, 45: 24442, 46: 25152, 47: 25738, 48: 26118, 49: 26327, 50: 26666, 51: 27065, 52: 27488, 53: 28144, 54: 28732, 55: 29335, 56: 29621, 57: 30571, 58: 30838, 59: 31585, 60: 32575, 61: 33401, 62: 34386, 63: 34570, 64: 35252, 65: 35524, 66: 36607, 67: 37030, 68: 37540, 69: 38132, 70: 38839, 71: 39123, 72: 39931, 73: 41079, 74: 41726, 75: 42471, 76: 43229, 77: 43938, 78: 44718, 79: 45200, 80: 45980, 81: 46793, 82: 47260, 83: 47670, 84: 48253, 85: 49016, 86: 49673, 87: 50684, 88: 51763, 89: 52202, 90: 52472, 91: 53367, 92: 53936, 93: 54773, 94: 55295, 95: 55844, 96: 56332, 97: 56748, 98: 57387, 99: 58166, 100: 58630, 101: 59689, 102: 60308, 103: 60814, 104: 61765, 105: 63027, 106: 63685, 107: 64168, 108: 64721, 109: 65139, 110: 65820, 111: 66566, 112: 66968, 113: 67911, 114: 68559, 115: 69054, 116: 69776, 117: 70257, 118: 70565, 119: 71037, 120: 71604, 121: 72609, 122: 73623, 123: 74144, 124: 74693, 125: 75609, 126: 76135, 127: 77013, 128: 77774, 129: 78503, 130: 79576, 131: 79900, 132: 80404, 133: 80956, 134: 81851, 135: 82403, 136: 82705, 137: 83447, 138: 84004, 139: 84504, 140: 85110, 141: 85969, 142: 86876, 143: 87831, 144: 88359, 145: 89016, 146: 89574, 147: 90252, 148: 90790, 149: 91240, 150: 91475, 151: 91741, 152: 92099, 153: 92590, 154: 93400, 155: 93939, 156: 94571, 157: 95269, 158: 95683, 159: 96166, 160: 96891, 161: 97593, 162: 98432, 163: 98906, 164: 100086, 165: 100675, 166: 100876, 167: 101568, 168: 101986, 169: 103336, 170: 104133, 171: 104635, 172: 105303, 173: 106530, 174: 107464, 175: 107810, 176: 108382, 177: 109179, 178: 110279, 179: 110912, 180: 111353, 181: 112172, 182: 112844, 183: 113183, 184: 113791, 185: 114559, 186: 115235, 187: 115990, 188: 116358, 189: 117202, 190: 117846, 191: 118426, 192: 119079, 193: 119553, 194: 120338, 195: 121329, 196: 122214, 197: 123018, 198: 123293, 199: 123559, 200: 124056, 201: 124351, 202: 124950, 203: 125969, 204: 126399, 205: 127275, 206: 128222, 207: 128704, 208: 129151, 209: 129924, 210: 130427, 211: 130981, 212: 131424, 213: 132053, 214: 133193, 215: 133960, 216: 134493, 217: 135361, 218: 135984, 219: 136746, 220: 137314, 221: 137981, 222: 138520, 223: 138887, 224: 139696, 225: 140431, 226: 140824, 227: 141079, 228: 141698, 229: 142074, 230: 142871, 231: 143455, 232: 144509, 233: 145187, 234: 145908, 235: 146822, 236: 147569, 237: 148458, 238: 148840, 239: 149406, 240: 149929, 241: 150868, 242: 151772, 243: 152732, 244: 153434, 245: 153986, 246: 154415, 247: 154618, 248: 154921, 249: 156014, 250: 156714, 251: 157280, 252: 157813, 253: 158684, 254: 159378, 255: 159594, 256: 160325, 257: 160903, 258: 161274, 259: 161821, 260: 162609, 261: 163063, 262: 164098, 263: 164934, 264: 165614, 265: 166237, 266: 166656, 267: 167005, 268: 167823, 269: 168409, 270: 169351, 271: 169751, 272: 170026, 273: 170698, 274: 171345, 275: 171765, 276: 172643, 277: 173214, 278: 174261, 279: 174806, 280: 175210, 281: 175937, 282: 176349, 283: 177334, 284: 178028, 285: 178755, 286: 179830, 287: 180050, 288: 180325, 289: 181013, 290: 181255, 291: 181590, 292: 182582, 293: 182888, 294: 183576, 295: 183760, 296: 184248, 297: 184474, 298: 184916, 299: 185795, 300: 186821, 301: 187603, 302: 187864, 303: 188244, 304: 188482, 305: 189596, 306: 190088, 307: 190847, 308: 191138, 309: 191793, 310: 192357, 311: 193056, 312: 193347, 313: 193877, 314: 194260, 315: 194974, 316: 195775, 317: 196467, 318: 197285, 319: 197815, 320: 198386, 321: 198811, 322: 199434, 323: 199942, 324: 200927, 325: 201870, 326: 202454, 327: 202897, 328: 203553, 329: 204406, 330: 205163, 331: 206080, 332: 206344, 333: 206684, 334: 207263, 335: 207605, 336: 207782, 337: 208664, 338: 209201, 339: 210330, 340: 211221, 341: 211946, 342: 212654, 343: 213386, 344: 213731, 345: 214700, 346: 215415, 347: 215943, 348: 216566, 349: 217214, 350: 217661, 351: 218045, 352: 218861, 353: 219149, 354: 219717, 355: 220184, 356: 220525, 357: 221434, 358: 221989, 359: 223053, 360: 223592}\n"
     ]
    }
   ],
   "source": [
    "mapframe_test = {}\n",
    "mapframe_test[-1] = 0\n",
    "for i in range(len(test)):\n",
    "    mapframe_test[i] = len(test[i]) + mapframe_test[i-1]\n",
    "    \n",
    "# print(mapframe_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test = {}\n",
    "len_test = len(test)\n",
    "k = 12\n",
    "\n",
    "for i in range(len_test):\n",
    "    \n",
    "    #corresponding to every index, store how many elements to pad and before(0) or after(1)\n",
    "    for j in range(mapframe_test[i-1], mapframe_test[i-1]+ k):\n",
    "        dict_test[j] = [k - (j - mapframe_test[i-1]), 0]  \n",
    "        \n",
    "    for j in range(mapframe_test[i]-k, mapframe_test[i]):\n",
    "        dict_test[j] = [1 + k- (mapframe_test[i] - j), 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class joinframe_test(Dataset):\n",
    "    \n",
    "    def __init__(self, x, k):\n",
    "        super().__init__()\n",
    "        self._x = x    \n",
    "        self.k = k\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._x)\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        \n",
    "        if index in dict_test :\n",
    "            len_pad = dict_test[index][0]\n",
    "            \n",
    "            if dict_test[index][1] == 0:\n",
    "                # before padding            \n",
    "                x_item = self._x[(index - (self.k - len_pad)) : (index + self.k+1)]\n",
    "                padding = np.zeros((dict_test[index][0], 40))\n",
    "                x_item = np.vstack((padding, x_item))\n",
    "                x_item = x_item.flatten()             \n",
    "          \n",
    "            elif dict_test[index][1] == 1:\n",
    "                # after padding\n",
    "                x_item = self._x[(index - self.k) : (index + 1 + (self.k-len_pad))]\n",
    "                padding = np.zeros((dict_test[index][0], 40))\n",
    "                x_item = np.vstack((x_item, padding))\n",
    "                x_item = x_item.flatten()\n",
    "                \n",
    "        else:\n",
    "            x_item = self._x[index - self.k : index + self.k + 1]\n",
    "            x_item = x_item.flatten()\n",
    "\n",
    "        return x_item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "total = 0\n",
    "correct = 0\n",
    "y = []\n",
    "start = time.time()\n",
    "\n",
    "frameobj = joinframe_test(concat_test, k = 12) \n",
    "framedata = DataLoader(frameobj, shuffle= False, pin_memory=True, num_workers=0)\n",
    "\n",
    "\n",
    "for xbatch in framedata:\n",
    "    runtime+=1\n",
    "    \n",
    "    xbatch = xbatch.to(device)\n",
    "    output = model(xbatch)\n",
    "    _, y_cap = torch.max(output, 1)\n",
    "    y.extend(y_cap.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=submission.csv>Download CSV file</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(np.array(y), columns=['label'])\n",
    "\n",
    "df.to_csv('submission.csv')\n",
    "\n",
    "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    html = '<a href={filename}>{title}</a>'\n",
    "    html = html.format(title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a link to download the dataframe which was saved with .to_csv method\n",
    "create_download_link(filename='submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
